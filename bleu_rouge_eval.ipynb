{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfrYb-h8GzMo"
      },
      "source": [
        "# BLUE and ROUGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA06GnkGHB6_"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65C0zUjRH682"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQJgI0LRH2oI"
      },
      "outputs": [],
      "source": [
        "# Helper functions\n",
        "def load_jsonl(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return [json.loads(line) for line in file.readlines()]\n",
        "\n",
        "def calculate_bleu_scores_with_smoothing(gpt4_descs, llava_descs):\n",
        "    smoothie = SmoothingFunction().method4\n",
        "    bleu_scores = []\n",
        "    for gpt4_text, llava_text in zip(gpt4_descs, llava_descs):\n",
        "        reference = [gpt4_text.split()]  # BLEU expects list of words\n",
        "        candidate = llava_text.split()\n",
        "        bleu_score = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
        "        bleu_scores.append(bleu_score)\n",
        "    return sum(bleu_scores) / len(bleu_scores)\n",
        "\n",
        "def lcs(x, y):\n",
        "    x = [xi.lower() for xi in x.split()]\n",
        "    y = [yi.lower() for yi in y.split()]\n",
        "    n, m = len(x), len(y)\n",
        "    L = [[0] * (m + 1) for _ in range(n + 1)]\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            if x[i] == y[j]:\n",
        "                L[i + 1][j + 1] = L[i][j] + 1\n",
        "            else:\n",
        "                L[i + 1][j + 1] = max(L[i + 1][j], L[i][j + 1])\n",
        "    return L[-1][-1]\n",
        "\n",
        "def rouge_l(reference, hypothesis):\n",
        "    lcs_len = lcs(reference, hypothesis)\n",
        "    if lcs_len == 0:\n",
        "        return 0  # Avoid division by zero\n",
        "    prec = lcs_len / len(hypothesis.split())\n",
        "    rec = lcs_len / len(reference.split())\n",
        "    f1 = 2 * prec * rec / (prec + rec)\n",
        "    return f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WgS2tJtIAuo"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "gpt4_data = load_jsonl('gpt4_response.jsonl')\n",
        "llava_data = load_jsonl('llava_response.jsonl')\n",
        "gpt4_descriptions = [line.strip() for line in gpt4_data]\n",
        "llava_dataset_sizes = [int(size) for entry in llava_data for size in entry.keys()]\n",
        "llava_descriptions = [entry[str(size)] for entry in llava_data for size in entry.keys()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qINHycoXICny"
      },
      "outputs": [],
      "source": [
        "# Calculate metrics\n",
        "bleu_scores = [calculate_bleu_scores_with_smoothing(gpt4_descriptions, desc) for desc in llava_descriptions]\n",
        "rouge_l_scores = [sum(rouge_l(gpt, llv) for gpt, llv in zip(gpt4_descriptions, desc)) / len(desc) for desc in llava_descriptions]\n",
        "\n",
        "# Create DataFrame\n",
        "metrics_data = {\n",
        "    'Dataset Size': llava_dataset_sizes,\n",
        "    'BLEU Score': bleu_scores,\n",
        "    'ROUGE-L Score': rouge_l_scores\n",
        "}\n",
        "metrics_df = pd.DataFrame(metrics_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZDd2qgV6IfN3"
      },
      "outputs": [],
      "source": [
        "metrics_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3Hz6Ip8ID60"
      },
      "outputs": [],
      "source": [
        "# Plot metrics - BLEU\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(llava_dataset_sizes, bleu_scores, marker='o', linestyle='-', color='b', label='BLEU Score')\n",
        "plt.title('BLEU Scores by Dataset Size')\n",
        "plt.xlabel('Dataset Size')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(llava_dataset_sizes)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47a0U135IoI0"
      },
      "outputs": [],
      "source": [
        "# Plot metrics\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(llava_dataset_sizes, rouge_l_scores, marker='o', linestyle='-', color='r', label='ROUGE-L Score')\n",
        "plt.title('ROUGE-L Scores by Dataset Size')\n",
        "plt.xlabel('Dataset Size')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.xticks(llava_dataset_sizes)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5vcLRjVqIFGi"
      },
      "outputs": [],
      "source": [
        "# Save the DataFrame to a pickle file\n",
        "pickle_path = 'metrics_scores.pkl'\n",
        "metrics_df.to_pickle(pickle_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzwCGDq2I2Os"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
