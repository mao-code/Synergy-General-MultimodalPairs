{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cDqfbXfE8E8G",
      "metadata": {
        "id": "cDqfbXfE8E8G"
      },
      "source": [
        "# Change Cache Directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "y5jYOXcIM91W8GIsltbPrblv",
      "metadata": {
        "executionInfo": {
          "elapsed": 397,
          "status": "ok",
          "timestamp": 1707495712938,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "y5jYOXcIM91W8GIsltbPrblv",
        "tags": []
      },
      "outputs": [],
      "source": [
        "# https://saturncloud.io/blog/how-to-change-huggingface-transformers-default-cache-directory-a-stepbystep-guide/\n",
        "import os\n",
        "\n",
        "os.environ['HF_HOME'] = '/content/cache'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MGFUX_wmP35c",
      "metadata": {
        "id": "MGFUX_wmP35c"
      },
      "source": [
        "# Initialization\n",
        "* WandB init"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S6VuXpRLP63V",
      "metadata": {
        "id": "S6VuXpRLP63V"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "072WVdDu7t1F",
      "metadata": {
        "id": "072WVdDu7t1F"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4PR3T6DHx4xv",
      "metadata": {
        "id": "4PR3T6DHx4xv"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ad3vfHdQJbB8",
      "metadata": {
        "id": "Ad3vfHdQJbB8"
      },
      "outputs": [],
      "source": [
        "!pip install bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fTbjqVGBRGxm",
      "metadata": {
        "id": "fTbjqVGBRGxm"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "od2894fMP-Wz",
      "metadata": {
        "executionInfo": {
          "elapsed": 1972,
          "status": "ok",
          "timestamp": 1707495715679,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "od2894fMP-Wz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import wandb\n",
        "import datetime\n",
        "import typing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m5rXRKqMQD10",
      "metadata": {
        "id": "m5rXRKqMQD10"
      },
      "outputs": [],
      "source": [
        "# Log in to WandB account\n",
        "wandb.login()\n",
        "\n",
        "# change model, dataset size, etc.\n",
        "\"\"\"\n",
        "model: LLaVA, MiniGPT4\n",
        "dataset_size: 1K, 2K, ..., 7K\n",
        "\"\"\"\n",
        "\n",
        "unique_id = datetime.datetime.now().strftime(\"%Y.%m.%d_%H.%M.%S\")\n",
        "\n",
        "# Initialize a new WandB run\n",
        "# wandb.init(project=\"synergistic-dataset\", name=unique_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vFhAc6_78YOh",
      "metadata": {
        "id": "vFhAc6_78YOh"
      },
      "source": [
        "# Load Dataset\n",
        "* generated by LLaVA-v1.3-13b, Vicuna-7b-v1.5 and SDXL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aOtVAlUNQFtb",
      "metadata": {
        "id": "aOtVAlUNQFtb"
      },
      "source": [
        "## Load"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8pl4GJzYh6H_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 6706,
          "status": "ok",
          "timestamp": 1707047825098,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "8pl4GJzYh6H_",
        "outputId": "3dce9f7a-78ce-413c-b6c3-5b5766132939"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qg6tByH0pitt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 64269,
          "status": "ok",
          "timestamp": 1707047889364,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "qg6tByH0pitt",
        "outputId": "e53b9faf-e22a-4949-f0b6-e6ed6a3c7772"
      },
      "outputs": [],
      "source": [
        "!git lfs install\n",
        "!git clone https://huggingface.co/datasets/MaoXun/Synergy-General-MultimodalPairs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fU-1pv3qq8sr",
      "metadata": {
        "id": "fU-1pv3qq8sr"
      },
      "outputs": [],
      "source": [
        "!mv Synergy-General-MultimodalPairs SDXL"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5rXAPne-QJnU",
      "metadata": {
        "id": "5rXAPne-QJnU"
      },
      "source": [
        "## Construct JSON"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dZ9AgcDsrAam",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 78076,
          "status": "ok",
          "timestamp": 1707047967423,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "dZ9AgcDsrAam",
        "outputId": "e25e19f5-11a8-4f6d-c602-98390d61de12"
      },
      "outputs": [],
      "source": [
        "# unzip each dataset\n",
        "import os\n",
        "from zipfile import ZipFile\n",
        "\n",
        "folder_path = '/content/SDXL'\n",
        "\n",
        "for filename in os.listdir(folder_path):\n",
        "    if filename.endswith(\".zip\"):\n",
        "        filepath = os.path.join(folder_path, filename)\n",
        "\n",
        "        with ZipFile(filepath, 'r') as zip_ref:\n",
        "            zip_ref.extractall(folder_path)\n",
        "            print(f\"Extracted: {filename}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4Rog2tqTshHF",
      "metadata": {
        "id": "4Rog2tqTshHF"
      },
      "outputs": [],
      "source": [
        "# remove zip files\n",
        "!rm -rf /content/SDXL/*.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IPClavwqS5es",
      "metadata": {
        "id": "IPClavwqS5es"
      },
      "outputs": [],
      "source": [
        "rounds = 7\n",
        "batches = [20, 20, 20, 20, 15, 20, 20]\n",
        "m = 10\n",
        "n = 5\n",
        "\n",
        "for r in range(rounds):\n",
        "  b = batches[r]\n",
        "\n",
        "  unzip_path = f\"{folder_path}/{r+1}_{b}_{m}_{n}\";\n",
        "\n",
        "  for file in os.listdir(unzip_path):\n",
        "    if file.endswith('.zip'):\n",
        "        zip_path = os.path.join(unzip_path, file)\n",
        "        with ZipFile(zip_path, 'r') as zip_ref:\n",
        "            zip_ref.extractall(unzip_path+'/images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z46HFPLwT56m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1893,
          "status": "ok",
          "timestamp": 1707048004719,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "Z46HFPLwT56m",
        "outputId": "787b2a30-ef9b-4677-d578-f2481ec59c7f"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "import json\n",
        "import ast\n",
        "\n",
        "\"\"\"\n",
        "for multi-round multi-batch datas,\n",
        "we organize to this way\n",
        "{\n",
        "    \"id\": \"{r}_{i}_{j}\",\n",
        "    \"image\": \"SDXL/{r}_{b}_{m}_{n}/images/{i}/{j}.jpg\",\n",
        "    \"conversations\": [\n",
        "      {\n",
        "        \"from\": \"human\",\n",
        "        \"value\": \"{I}\\n<image>\"\n",
        "      },\n",
        "      {\n",
        "        \"from\": \"gpt\",\n",
        "        \"value\": \"{ans}\"\n",
        "      }\n",
        "    ],\n",
        "    ...\n",
        "}\n",
        "\"\"\"\n",
        "rounds = 7\n",
        "batches = [20, 20, 20, 20, 15, 20, 20]\n",
        "m = 10\n",
        "n = 5\n",
        "\n",
        "fixed_instruction = \"Please describe this image in detail.\"\n",
        "\n",
        "datas = []\n",
        "for r in range(rounds):\n",
        "    b = batches[r]\n",
        "    base_file_path = f\"/content/SDXL/{r+1}_{b}_{m}_{n}\";\n",
        "    S_path = f\"{base_file_path}/S.csv\"\n",
        "    S_df = pd.read_csv(S_path)\n",
        "\n",
        "    for i, row in S_df.iterrows():\n",
        "      for j, (column, value) in enumerate(row.items()):\n",
        "        # ignore the D_0\n",
        "        if column == \"D_init\":\n",
        "          continue\n",
        "\n",
        "        value = ast.literal_eval(value)\n",
        "        img, output = value\n",
        "        img = img.replace(\"/content/images/\",\"\")\n",
        "\n",
        "        img = f\"{base_file_path}/images/{img}\"\n",
        "\n",
        "        data = {\n",
        "            \"id\": f\"{r+1}_{b}_{i}_{j}\",\n",
        "            \"conversations\": [\n",
        "                { \"from\": \"human\", \"value\": f\"{fixed_instruction}\\n<image>\"},\n",
        "                { \"from\": \"gpt\", \"value\": output}\n",
        "            ],\n",
        "            \"image\": img,\n",
        "        }\n",
        "\n",
        "        datas.append(data)\n",
        "\n",
        "# convert it to json file\n",
        "file_name = \"data_lora.json\"\n",
        "with open(file_name, \"w\") as json_file:\n",
        "    json.dump(datas, json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qnITSthcT7f4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1707048004719,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "qnITSthcT7f4",
        "outputId": "d8d896eb-7d7b-425e-c4d3-2eaaefa86bb8"
      },
      "outputs": [],
      "source": [
        "datas[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C6onP-miQS3F",
      "metadata": {
        "id": "C6onP-miQS3F"
      },
      "source": [
        "## Helper Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K8P7Le4UQU-_",
      "metadata": {
        "id": "K8P7Le4UQU-_"
      },
      "outputs": [],
      "source": [
        "def save_data_by_size(size):\n",
        "  \"\"\"\n",
        "  split the data by size\n",
        "  e.g. size = 3000 -> save 3000 entries to train_data_lora.json\n",
        "  \"\"\"\n",
        "  with open(\"/content/data_lora.json\", \"r\") as json_file:\n",
        "    datas = json.load(json_file)\n",
        "\n",
        "  train_datas = datas[:size]\n",
        "\n",
        "  with open(\"/content/train_data_lora.json\", \"w\") as json_file:\n",
        "    json.dump(train_datas, json_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "r2Y_0yuv8Oaz",
      "metadata": {
        "id": "r2Y_0yuv8Oaz"
      },
      "source": [
        "# Train and Evaluate Models on Different Size of this Dataset\n",
        "* 1K, 2K, ..., 7K\n",
        "* training and evaluating\n",
        "* log response to WandB\n",
        "---\n",
        "* Models:\n",
        "  * LLaVA\n",
        "  * MiniGPT4\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "F14QCkbzQSGo",
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1707472157137,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "F14QCkbzQSGo"
      },
      "outputs": [],
      "source": [
        "size = [1000, 2000, 3000, 4000, 5000, 6000, 7000]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sdhzm2oBPn1z",
      "metadata": {
        "id": "Sdhzm2oBPn1z"
      },
      "source": [
        "## LLaVA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QCuwKVdXSJLt",
      "metadata": {
        "id": "QCuwKVdXSJLt"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lBMo0xpYSInl",
      "metadata": {
        "id": "lBMo0xpYSInl"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/haotian-liu/LLaVA.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8VmP3r6PyDd2",
      "metadata": {
        "id": "8VmP3r6PyDd2"
      },
      "outputs": [],
      "source": [
        "!cd /content/LLaVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Kbr-1NgySN06",
      "metadata": {
        "id": "Kbr-1NgySN06"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jUA7q0HVSWQZ",
      "metadata": {
        "id": "jUA7q0HVSWQZ"
      },
      "outputs": [],
      "source": [
        "!pip install -e \".[train]\"\n",
        "!pip install flash-attn --no-build-isolation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0Pd2Da-BSKxc",
      "metadata": {
        "id": "0Pd2Da-BSKxc"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_OhBvcYlTm6R",
      "metadata": {
        "id": "_OhBvcYlTm6R"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u15o2pKqUCx6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 644,
          "status": "ok",
          "timestamp": 1707048453362,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "u15o2pKqUCx6",
        "outputId": "f99838c3-d6ef-4bf8-d1d0-9096b437fca1"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "%mkdir checkpoints\n",
        "%cd checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mvcq4fOrUKYs",
      "metadata": {
        "id": "mvcq4fOrUKYs"
      },
      "outputs": [],
      "source": [
        "# download multimodal projecter\n",
        "!wget https://huggingface.co/liuhaotian/llava-336px-pretrain-vicuna-7b-v1.3/resolve/main/mm_projector.bin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AKXc2VRuDYqq",
      "metadata": {
        "id": "AKXc2VRuDYqq"
      },
      "outputs": [],
      "source": [
        "unique_id = datetime.datetime.now().strftime(\"%Y.%m.%d_%H.%M.%S\")\n",
        "\n",
        "config = {\n",
        "    \"model\": \"LLaVA\",\n",
        "    \"dataset_size\": 3000,\n",
        "    \"project_name\": \"synergistic-dataset\",\n",
        "    \"name\": unique_id\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nO1KLNV6_v8B",
      "metadata": {
        "id": "nO1KLNV6_v8B"
      },
      "outputs": [],
      "source": [
        "os.environ[\"WANDB_API_KEY\"] = \"c024ab583d4aab78b0ca7f9329635c7843aea5e2\"\n",
        "os.environ[\"WANDB_ENTITY\"] = \"codingmaoxun\"\n",
        "os.environ[\"WANDB_PROJECT\"] = config[\"project_name\"]\n",
        "os.environ[\"WANDB_NAME\"] = config[\"name\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jWTdFwpBTWk_",
      "metadata": {
        "id": "jWTdFwpBTWk_"
      },
      "outputs": [],
      "source": [
        "# using LLaVA LoRA fine-tune script\n",
        "# 1. freeze the original weights\n",
        "# 2. calculate trainable weights and split into A, B\n",
        "# 3. training the LoRA weights\n",
        "# 4. upload lora weights\n",
        "\n",
        "# note:\n",
        "# from LLaVA page, it seems to fine-tune in two stages\n",
        "# LLaVa connects pre-trained CLIP ViT-L/14 visual encoder and large language model Vicuna, using a simple projection matrix. We consider a two-stage instruction-tuning procedure:\n",
        "# Stage 1: Pre-training for Feature Alignment. Only the projection matrix is updated, based on a subset of CC3M.\n",
        "# Stage 2: Fine-tuning End-to-End. Both the projection matrix and LLM are updated for two different use senarios:\n",
        "\n",
        "\"\"\"\n",
        "config:\n",
        "    dataset_size\n",
        "\"\"\"\n",
        "\n",
        "def write_fine_tune_script(bash_script_path, config):\n",
        "    bash_script_content = f\"\"\"#!/bin/bash\n",
        "################## VICUNA ##################\n",
        "PROMPT_VERSION=v1\n",
        "MODEL_VERSION=\"vicuna-7b-v1.3\"\n",
        "################## VICUNA ##################\n",
        "\n",
        "deepspeed /content/LLaVA/llava/train/train_mem.py \\\n",
        "    --deepspeed /content/LLaVA/scripts/zero2.json \\\n",
        "    --lora_enable True \\\n",
        "    --model_name_or_path $MODEL_VERSION \\\n",
        "    --version $PROMPT_VERSION \\\n",
        "    --data_path /content/train_data_lora.json \\\n",
        "    --image_folder \"\" \\\n",
        "    --vision_tower openai/clip-vit-large-patch14 \\\n",
        "    --pretrain_mm_mlp_adapter /content/checkpoints/mm_projector.bin \\\n",
        "    --mm_vision_select_layer -2 \\\n",
        "    --mm_use_im_start_end False \\\n",
        "    --mm_use_im_patch_token False \\\n",
        "    --bf16 True \\\n",
        "    --output_dir /content/checkpoints/llava-$MODEL_VERSION-{config[\"dataset_size\"]}-finetune_lora \\\n",
        "    --num_train_epochs 1 \\\n",
        "    --per_device_train_batch_size 16 \\\n",
        "    --per_device_eval_batch_size 4 \\\n",
        "    --gradient_accumulation_steps 1 \\\n",
        "    --evaluation_strategy \"no\" \\\n",
        "    --save_strategy \"steps\" \\\n",
        "    --save_steps 50000 \\\n",
        "    --save_total_limit 1 \\\n",
        "    --learning_rate 2e-5 \\\n",
        "    --weight_decay 0. \\\n",
        "    --warmup_ratio 0.03 \\\n",
        "    --lr_scheduler_type \"cosine\" \\\n",
        "    --logging_steps 1 \\\n",
        "    --tf32 True \\\n",
        "    --model_max_length 2048 \\\n",
        "    --gradient_checkpointing True \\\n",
        "    --lazy_preprocess True \\\n",
        "    --dataloader_num_workers 4 \\\n",
        "    --report_to wandb\n",
        "\"\"\"\n",
        "\n",
        "    with open(bash_script_path, 'w') as file:\n",
        "        file.write(bash_script_content)\n",
        "        os.chmod(bash_script_path, 0o755)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jrwQtIx-8ZQZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 417,
          "status": "ok",
          "timestamp": 1707116184417,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "jrwQtIx-8ZQZ",
        "outputId": "c40b036b-3ebb-4d82-ec3e-a16a20744c35"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pfYNyMfNBb_C",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1707116185063,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "pfYNyMfNBb_C",
        "outputId": "49409640-3d38-4682-9ec3-228984356463"
      },
      "outputs": [],
      "source": [
        "%cd /content/LLaVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t_xleZaiUgLC",
      "metadata": {
        "id": "t_xleZaiUgLC"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import json\n",
        "\n",
        "\"\"\"\n",
        "The final lora weights is at \"/content/checkpoints/llava-$MODEL_VERSION-finetune_lora\"\n",
        "\"\"\"\n",
        "\n",
        "# save different data size to train_data_lora.json\n",
        "size = [1000, 2000, 3000, 4000, 5000, 6000, 7000]\n",
        "bash_path = \"/content/lora_fine_tune.sh\"\n",
        "\n",
        "for i in range(len(size)):\n",
        "    write_fine_tune_script(bash_path, {\n",
        "        \"dataset_size\": size[i]\n",
        "    })\n",
        "    save_data_by_size(size[i])\n",
        "\n",
        "    # set new wandb environ\n",
        "    os.environ[\"WANDB_NAME\"] = config[\"name\"]+\"_size_\"+str(size[i])\n",
        "\n",
        "    result = subprocess.run([bash_path], capture_output=True, text=True, shell=True)\n",
        "\n",
        "    # Print the standard output and error (if any)\n",
        "    print(\"STDOUT:\", result.stdout)\n",
        "    print(\"STDERR:\", result.stderr)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ju0wKZftUrAL",
      "metadata": {
        "id": "Ju0wKZftUrAL"
      },
      "source": [
        "### Get the 100 image response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oyh4Dh-UybyK",
      "metadata": {
        "executionInfo": {
          "elapsed": 631,
          "status": "ok",
          "timestamp": 1707472164294,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "oyh4Dh-UybyK"
      },
      "outputs": [],
      "source": [
        "fixed_instruction = \"Please describe the image in detail.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jUaG8TYI2S3q",
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1707472164898,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "jUaG8TYI2S3q"
      },
      "outputs": [],
      "source": [
        "# save response path\n",
        "response_base_bath = \"/content/response\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q3ySkF8nPeWF",
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1707472165336,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "q3ySkF8nPeWF"
      },
      "outputs": [],
      "source": [
        "img_paths = [\n",
        "    f\"/content/testing/random_images/image_{i}.jpg\"\n",
        "    for i in range(100)\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sAIXxBgQyLed",
      "metadata": {
        "id": "sAIXxBgQyLed"
      },
      "source": [
        "### LLaVA Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Oo9hQUgvSFar",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 13135,
          "status": "ok",
          "timestamp": 1707472182302,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "Oo9hQUgvSFar",
        "outputId": "8d0cbabb-67db-4794-8b45-304ab5b42879"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade pip setuptools wheel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LVihbaT0RZvB",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 6742,
          "status": "ok",
          "timestamp": 1707472189041,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "LVihbaT0RZvB",
        "outputId": "4079b298-e3ca-46b7-b7d4-93f912eb9673"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jgoJUhRATAWD",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1707472189041,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "jgoJUhRATAWD",
        "outputId": "08c0973a-731e-48a1-ccf1-0324ff3ca98a"
      },
      "outputs": [],
      "source": [
        "%cd /content/LLaVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FBy-S3A6yQ1o",
      "metadata": {
        "executionInfo": {
          "elapsed": 30128,
          "status": "ok",
          "timestamp": 1707472219166,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "FBy-S3A6yQ1o"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import torch\n",
        "\n",
        "from llava.constants import (\n",
        "    IMAGE_TOKEN_INDEX,\n",
        "    DEFAULT_IMAGE_TOKEN,\n",
        "    DEFAULT_IM_START_TOKEN,\n",
        "    DEFAULT_IM_END_TOKEN,\n",
        "    IMAGE_PLACEHOLDER,\n",
        ")\n",
        "from llava.conversation import conv_templates, SeparatorStyle\n",
        "from llava.model.builder import load_pretrained_model\n",
        "from llava.utils import disable_torch_init\n",
        "from llava.mm_utils import (\n",
        "    process_images,\n",
        "    tokenizer_image_token,\n",
        "    get_model_name_from_path,\n",
        ")\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import re\n",
        "\n",
        "\n",
        "def image_parser(args):\n",
        "    out = args.image_file.split(args.sep)\n",
        "    return out\n",
        "\n",
        "\n",
        "def load_image(image_file):\n",
        "    if image_file.startswith(\"http\") or image_file.startswith(\"https\"):\n",
        "        response = requests.get(image_file)\n",
        "        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n",
        "    else:\n",
        "        image = Image.open(image_file).convert(\"RGB\")\n",
        "    return image\n",
        "\n",
        "\n",
        "def load_images(image_files):\n",
        "    out = []\n",
        "    for image_file in image_files:\n",
        "        image = load_image(image_file)\n",
        "        out.append(image)\n",
        "    return out\n",
        "\n",
        "\n",
        "def eval_model(args):\n",
        "    # Model\n",
        "    disable_torch_init()\n",
        "\n",
        "    model_name = get_model_name_from_path(args.model_path)\n",
        "    tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
        "        args.model_path, args.model_base, model_name\n",
        "    )\n",
        "\n",
        "    qs = args.query\n",
        "    image_token_se = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN\n",
        "    if IMAGE_PLACEHOLDER in qs:\n",
        "        if model.config.mm_use_im_start_end:\n",
        "            qs = re.sub(IMAGE_PLACEHOLDER, image_token_se, qs)\n",
        "        else:\n",
        "            qs = re.sub(IMAGE_PLACEHOLDER, DEFAULT_IMAGE_TOKEN, qs)\n",
        "    else:\n",
        "        if model.config.mm_use_im_start_end:\n",
        "            qs = image_token_se + \"\\n\" + qs\n",
        "        else:\n",
        "            qs = DEFAULT_IMAGE_TOKEN + \"\\n\" + qs\n",
        "\n",
        "    if \"llama-2\" in model_name.lower():\n",
        "        conv_mode = \"llava_llama_2\"\n",
        "    elif \"mistral\" in model_name.lower():\n",
        "        conv_mode = \"mistral_instruct\"\n",
        "    elif \"v1.6-34b\" in model_name.lower():\n",
        "        conv_mode = \"chatml_direct\"\n",
        "    elif \"v1\" in model_name.lower():\n",
        "        conv_mode = \"llava_v1\"\n",
        "    elif \"mpt\" in model_name.lower():\n",
        "        conv_mode = \"mpt\"\n",
        "    else:\n",
        "        conv_mode = \"llava_v0\"\n",
        "\n",
        "    if args.conv_mode is not None and conv_mode != args.conv_mode:\n",
        "        print(\n",
        "            \"[WARNING] the auto inferred conversation mode is {}, while `--conv-mode` is {}, using {}\".format(\n",
        "                conv_mode, args.conv_mode, args.conv_mode\n",
        "            )\n",
        "        )\n",
        "    else:\n",
        "        args.conv_mode = conv_mode\n",
        "\n",
        "    conv = conv_templates[args.conv_mode].copy()\n",
        "    conv.append_message(conv.roles[0], qs)\n",
        "    conv.append_message(conv.roles[1], None)\n",
        "    prompt = conv.get_prompt()\n",
        "\n",
        "    image_files = image_parser(args)\n",
        "    images = load_images(image_files)\n",
        "    image_sizes = [x.size for x in images]\n",
        "    images_tensor = process_images(\n",
        "        images,\n",
        "        image_processor,\n",
        "        model.config\n",
        "    ).to(model.device, dtype=torch.float16)\n",
        "\n",
        "    input_ids = (\n",
        "        tokenizer_image_token(prompt, tokenizer, IMAGE_TOKEN_INDEX, return_tensors=\"pt\")\n",
        "        .unsqueeze(0)\n",
        "        .cuda()\n",
        "    )\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        output_ids = model.generate(\n",
        "            input_ids,\n",
        "            images=images_tensor,\n",
        "            image_sizes=image_sizes,\n",
        "            do_sample=True if args.temperature > 0 else False,\n",
        "            temperature=args.temperature,\n",
        "            top_p=args.top_p,\n",
        "            num_beams=args.num_beams,\n",
        "            max_new_tokens=args.max_new_tokens,\n",
        "            use_cache=True,\n",
        "        )\n",
        "\n",
        "    outputs = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()\n",
        "\n",
        "    return outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CZE7fH8lyXxZ",
      "metadata": {
        "executionInfo": {
          "elapsed": 4,
          "status": "ok",
          "timestamp": 1707472219166,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "CZE7fH8lyXxZ"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def generate_description_of_image(\n",
        "    lora_model_path: str,\n",
        "    model_base: str,\n",
        "    img_paths: List[str],\n",
        ") -> str:\n",
        "  model_name = get_model_name_from_path(lora_model_path)\n",
        "  prompt = fixed_instruction\n",
        "  imageFiles = \" \".join(img_paths)\n",
        "\n",
        "  args = type('Args', (), {\n",
        "      \"model_path\": lora_model_path,\n",
        "      \"model_base\": model_base,\n",
        "      \"model_name\": model_name,\n",
        "      \"query\": prompt,\n",
        "      \"conv_mode\": None,\n",
        "      \"image_file\": imageFiles,\n",
        "      \"sep\": \" \",\n",
        "      \"temperature\": 0.7,\n",
        "      \"top_p\": 0.9,\n",
        "      \"num_beams\": 3,\n",
        "      \"max_new_tokens\": 150,\n",
        "  })()\n",
        "\n",
        "  outputs = eval_model(args)\n",
        "  print(outputs)\n",
        "\n",
        "  return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PIBBvcCtyNN-",
      "metadata": {
        "id": "PIBBvcCtyNN-"
      },
      "source": [
        "### Merge LoRA and Generate Response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JoezlhPHP6Cx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1707475947556,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "JoezlhPHP6Cx",
        "outputId": "b976636d-bbc6-4753-e0be-67e0d27dbc7b"
      },
      "outputs": [],
      "source": [
        "%cd /content/LLaVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mDC1NpJ3bfyY",
      "metadata": {
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1707475948838,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "mDC1NpJ3bfyY"
      },
      "outputs": [],
      "source": [
        "from llava.model.builder import load_pretrained_model\n",
        "from llava.mm_utils import get_model_name_from_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M2e_WuHbwrea",
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1707475950066,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "M2e_WuHbwrea"
      },
      "outputs": [],
      "source": [
        "def construct_llava_lora_model_path(datasize):\n",
        "  lora_model_path = f\"/content/checkpoints/llava-vicuna-7b-v1.3-{datasize}-finetune_lora\"\n",
        "  return lora_model_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efJ9BRvixQx5",
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "ok",
          "timestamp": 1707475950066,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "efJ9BRvixQx5"
      },
      "outputs": [],
      "source": [
        "llava_response = {\n",
        "    \"1000\": [],\n",
        "    \"2000\": [],\n",
        "    \"3000\": [],\n",
        "    \"4000\": [],\n",
        "    \"5000\": [],\n",
        "    \"6000\": [],\n",
        "    \"7000\": []\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nkNz_zsVUwG-",
      "metadata": {
        "id": "nkNz_zsVUwG-"
      },
      "outputs": [],
      "source": [
        "model_base = \"lmsys/vicuna-7b-v1.3\"\n",
        "\n",
        "for i in range(len(size)):\n",
        "  lora_model_path = construct_llava_lora_model_path(size[i])\n",
        "\n",
        "  for img_path in img_paths:\n",
        "    # generate response and store\n",
        "    response = generate_description_of_image(\n",
        "        lora_model_path = lora_model_path,\n",
        "        model_base = model_base,\n",
        "        img_paths = [img_path]\n",
        "    )\n",
        "\n",
        "    llava_response[str(size[i])].append(response)\n",
        "\n",
        "  # save model\n",
        "  # save_model_path = \"/content/checkpoints/checkpoint-llava-7-20-10-5-vicuna-7b-v1.3/\"\n",
        "  # model.save_pretrained(save_model_path)\n",
        "  # tokenizer.save_pretrained(save_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3daGA1jRoGmn",
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "aborted",
          "timestamp": 1707479413091,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "3daGA1jRoGmn"
      },
      "outputs": [],
      "source": [
        "print(llava_response[\"1000\"][40]+\"\\n\"+llava_response[\"7000\"][40])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vBz8WktMZCg_",
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "aborted",
          "timestamp": 1707479413091,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "vBz8WktMZCg_"
      },
      "outputs": [],
      "source": [
        "len(llava_response[\"1000\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8DkAhwR52DuF",
      "metadata": {
        "executionInfo": {
          "elapsed": 1,
          "status": "aborted",
          "timestamp": 1707479413091,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "8DkAhwR52DuF"
      },
      "outputs": [],
      "source": [
        "# save response to jsonl\n",
        "import json\n",
        "\n",
        "llava_response_folder = f\"{response_base_bath}/llava\"\n",
        "llava_response_path = f\"{response_base_bath}/llava/llava_response.jsonl\"\n",
        "os.makedirs(llava_response_folder, exist_ok=True)\n",
        "\n",
        "with open(llava_response_path, \"w\") as f:\n",
        "    for key, value in llava_response.items():\n",
        "        json_obj = {key: value}\n",
        "        json_str = json.dumps(json_obj)\n",
        "        f.write(json_str + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IOoCIQsXTODH",
      "metadata": {
        "id": "IOoCIQsXTODH"
      },
      "source": [
        "## MiniGPT4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D7GsLM6-TP1b",
      "metadata": {
        "id": "D7GsLM6-TP1b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OdEldhei2ooX",
      "metadata": {
        "id": "OdEldhei2ooX"
      },
      "outputs": [],
      "source": [
        "# save response\n",
        "minigpt4_response_path = f\"{response_base_bath}/minigpt4/minigpt4_response.jsonl\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "os5xedx28ln4",
      "metadata": {
        "id": "os5xedx28ln4"
      },
      "source": [
        "# Compute the BERTScore with Benchmark Model\n",
        "* run the downloading 100 images first\n",
        "* benchmark model: GPT4(need money), LLaVA-34B-v1.6(too big), LLaVA-13B-v1.6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vCtUUedWPs2a",
      "metadata": {
        "id": "vCtUUedWPs2a"
      },
      "source": [
        "## GPT4-vision Generate Response (COST!!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bjkueDsj0z0l",
      "metadata": {
        "executionInfo": {
          "elapsed": 180,
          "status": "ok",
          "timestamp": 1707474387305,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "bjkueDsj0z0l"
      },
      "outputs": [],
      "source": [
        "# 100 images from GPT4 responses\n",
        "# pasted by asking GPT4\n",
        "gpt_responses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SAW__WxwPxQX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 1059232,
          "status": "ok",
          "timestamp": 1707475446904,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "SAW__WxwPxQX",
        "outputId": "9e15c921-fdcb-4572-9884-9fbfd2b9ed59"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "import requests\n",
        "\n",
        "# OpenAI API Key\n",
        "api_key = \"YOUR OPEN AI TOKEN\"\n",
        "\n",
        "headers = {\n",
        "  \"Content-Type\": \"application/json\",\n",
        "  \"Authorization\": f\"Bearer {api_key}\"\n",
        "}\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "  with open(image_path, \"rb\") as image_file:\n",
        "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
        "\n",
        "for img_path in img_paths:\n",
        "  # Getting the base64 string\n",
        "  base64_image = encode_image(img_path)\n",
        "\n",
        "\n",
        "  payload = {\n",
        "    \"model\": \"gpt-4-vision-preview\",\n",
        "    \"messages\": [\n",
        "      {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "          {\n",
        "            \"type\": \"text\",\n",
        "            \"text\": \"Please describe the image in detail\"\n",
        "          },\n",
        "          {\n",
        "            \"type\": \"image_url\",\n",
        "            \"image_url\": {\n",
        "              \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n",
        "            }\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    ],\n",
        "    \"max_tokens\": 1000\n",
        "  }\n",
        "\n",
        "  response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=headers, json=payload)\n",
        "  data = response.json()\n",
        "  response_text = data[\"choices\"][0][\"message\"][\"content\"]\n",
        "  gpt_responses.append(response_text)\n",
        "\n",
        "  print(f\"Usage: { data['usage'] }\")\n",
        "  print(f\"Preview response: { response_text[:20] }\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H-33qxLaYQOd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 12,
          "status": "ok",
          "timestamp": 1707475446904,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "H-33qxLaYQOd",
        "outputId": "91069889-cc1c-464c-a43f-76ea0600078f"
      },
      "outputs": [],
      "source": [
        "len(gpt_responses), gpt_responses[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "s6lwLpaxWgrd",
      "metadata": {
        "executionInfo": {
          "elapsed": 2,
          "status": "ok",
          "timestamp": 1707475446904,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "s6lwLpaxWgrd"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "gpt4_response_folder = f\"{response_base_bath}/gpt4\"\n",
        "gpt4_response_path = f\"{response_base_bath}/gpt4/gpt4_response.jsonl\"\n",
        "os.makedirs(gpt4_response_folder, exist_ok=True)\n",
        "\n",
        "with open(gpt4_response_path, 'w') as file:\n",
        "    for string in gpt_responses:\n",
        "        json_object = json.dumps(string)\n",
        "        file.write(json_object + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nC_ZPQ6LPxhd",
      "metadata": {
        "id": "nC_ZPQ6LPxhd"
      },
      "source": [
        "## Helper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q-XEwfMceaUl",
      "metadata": {
        "executionInfo": {
          "elapsed": 458,
          "status": "ok",
          "timestamp": 1707475586249,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "Q-XEwfMceaUl"
      },
      "outputs": [],
      "source": [
        "def read_jsonl_to_list(file_path):\n",
        "    data_list = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            data_list.append(json.loads(line))\n",
        "    return data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7AbnEPGYeaAt",
      "metadata": {
        "executionInfo": {
          "elapsed": 859,
          "status": "ok",
          "timestamp": 1707479685067,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "7AbnEPGYeaAt"
      },
      "outputs": [],
      "source": [
        "def read_jsonl_to_dict(file_path):\n",
        "    data_dict = {}\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            json_obj = json.loads(line)\n",
        "            for k, v in json_obj.items():\n",
        "              data_dict[k] = v\n",
        "    return data_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9TUdbk-HJWYK",
      "metadata": {
        "executionInfo": {
          "elapsed": 184,
          "status": "ok",
          "timestamp": 1707480877836,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "9TUdbk-HJWYK"
      },
      "outputs": [],
      "source": [
        "from bert_score import BERTScorer\n",
        "\n",
        "def calculate_bertscore(cands, refs):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    scorer = BERTScorer(\"microsoft/deberta-xlarge-mnli\", use_fast_tokenizer=True, device=device)\n",
        "\n",
        "    precision, recall, f1 = scorer.score(cands, refs)\n",
        "\n",
        "    # use recall score as reward\n",
        "    return recall.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fvly0b7hTIx5",
      "metadata": {
        "id": "Fvly0b7hTIx5"
      },
      "source": [
        "## LLaVA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qOv4lyvFeNyT",
      "metadata": {
        "id": "qOv4lyvFeNyT"
      },
      "outputs": [],
      "source": [
        "# read jsonl\n",
        "llava_responses = read_jsonl_to_dict(\"/content/response/llava/llava_response.jsonl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pttD7Dd7uku4",
      "metadata": {
        "executionInfo": {
          "elapsed": 393,
          "status": "ok",
          "timestamp": 1707480681973,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "pttD7Dd7uku4"
      },
      "outputs": [],
      "source": [
        "gpt_responses = read_jsonl_to_list(\"/content/response/gpt4/gpt4_response.jsonl\")\n",
        "gpt_responses = [gpt_response.replace('\\n',' ') for gpt_response in gpt_responses]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0xmusClsuqHz",
      "metadata": {
        "executionInfo": {
          "elapsed": 226,
          "status": "ok",
          "timestamp": 1707481051925,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "0xmusClsuqHz"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "bert_score_llava_gpt = defaultdict(list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8rEOM4gTRCc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 6,
          "status": "ok",
          "timestamp": 1707483232274,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "e8rEOM4gTRCc",
        "outputId": "b4691177-6579-4619-ad56-d02637006fe2"
      },
      "outputs": [],
      "source": [
        "for size, llava_res in llava_responses.items():\n",
        "  print(f\"processing size {size}...\")\n",
        "  for i in range(len(llava_res)):\n",
        "    # truncation\n",
        "    ref = gpt_responses[i]\n",
        "    cand = llava_res[i]\n",
        "\n",
        "    score = calculate_bertscore([cand], [ref])[0]\n",
        "    bert_score_llava_gpt[size].append(score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6Xd2jERPqsHB",
      "metadata": {
        "executionInfo": {
          "elapsed": 382,
          "status": "ok",
          "timestamp": 1707495646547,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "6Xd2jERPqsHB"
      },
      "outputs": [],
      "source": [
        "# load to pickle\n",
        "import pickle\n",
        "\n",
        "with open('/content/response/score/llava_score.pickle', 'wb') as file:\n",
        "    pickle.dump(bert_score_llava_gpt, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EORgIZWVq_lL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 395,
          "status": "ok",
          "timestamp": 1707495762494,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "EORgIZWVq_lL",
        "outputId": "b57534d5-4934-4504-bfe5-7ff80b89135d"
      },
      "outputs": [],
      "source": [
        "# get from pickle\n",
        "import pickle\n",
        "\n",
        "with open('/content/response/score/llava_score.pickle', 'rb') as file:\n",
        "    bert_score_llava_gpt = pickle.load(file)\n",
        "\n",
        "print(bert_score_llava_gpt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m2BT4evYyu8m",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 3,
          "status": "ok",
          "timestamp": 1707495764990,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "m2BT4evYyu8m",
        "outputId": "5b27af1d-5ea3-46da-f009-b1ba2910b956"
      },
      "outputs": [],
      "source": [
        "for size, scores in bert_score_llava_gpt.items():\n",
        "  print(f\"Dataset Size: {size}\\n\")\n",
        "  print(f\"Mean: {np.mean(scores)}\")\n",
        "  print(f\"Std: {np.std(scores)}\")\n",
        "  print(\"=\"*10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cNqiju_R2-ga",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "executionInfo": {
          "elapsed": 2198,
          "status": "ok",
          "timestamp": 1707496472133,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "cNqiju_R2-ga",
        "outputId": "fd26377b-4505-4e87-e5de-fc192576482b"
      },
      "outputs": [],
      "source": [
        "# log BERTScore to wandb\n",
        "name = \"llava_gpt_bert_score\"\n",
        "wandb.init(\n",
        "    project=\"synergistic-dataset\",\n",
        "    entity=\"codingmaoxun\",\n",
        "    name=name\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q5oKcvj0OgW_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210,
          "referenced_widgets": [
            "59ec0ba9df724a02955de65de116901c",
            "a87220be8ade4c62a0e891f32ea8076e",
            "63f30df580a8438997fe6c96e60cca8d",
            "224c80a990fe45a38bcad9f536742970",
            "32d5f76b89e74c6d88205ed1015a34c5",
            "98be374439484985b7dd5e5fb53a88ff",
            "b56695160b084e2d9c3ca920a2c10621",
            "fdbd2c368f7c4e26ac3cdf8c0247fe37"
          ]
        },
        "executionInfo": {
          "elapsed": 3551,
          "status": "ok",
          "timestamp": 1707496475674,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "Q5oKcvj0OgW_",
        "outputId": "1ad76104-0ca9-43ac-ec77-2ca54c649f8f"
      },
      "outputs": [],
      "source": [
        "# Compute mean scores and standard deviations\n",
        "mean_scores = []\n",
        "std_devs = []\n",
        "for dataset_size, scores in bert_score_llava_gpt.items():\n",
        "    mean_score = np.mean(scores)\n",
        "    std_dev = np.std(scores)\n",
        "\n",
        "    mean_scores.append(mean_score)\n",
        "    std_devs.append(std_dev)\n",
        "\n",
        "dataset_sizes = [int(key) for key in list(bert_score_llava_gpt.keys())]\n",
        "\n",
        "for i in range(len(dataset_sizes)):\n",
        "  wandb.log(\n",
        "      {\n",
        "        \"llava_score/mean\": mean_scores[i],\n",
        "        \"llava_score/std\": std_devs[i],\n",
        "      },\n",
        "      step=dataset_sizes[i]\n",
        "  )\n",
        "\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cRJszAdATJlq",
      "metadata": {
        "id": "cRJszAdATJlq"
      },
      "source": [
        "## MiniGPT4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m0GVriBHTKT5",
      "metadata": {
        "id": "m0GVriBHTKT5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "iDFUGBwW1Wvt",
      "metadata": {
        "id": "iDFUGBwW1Wvt"
      },
      "source": [
        "# Download 100 testing images (Executed Once)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IHwcjnbJOx2a",
      "metadata": {
        "id": "IHwcjnbJOx2a"
      },
      "outputs": [],
      "source": [
        "folder_path = os.path.join(\"/content/testing/random_images\")\n",
        "\n",
        "os.makedirs(folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oX--Ye5l1Yuq",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "executionInfo": {
          "elapsed": 48248,
          "status": "ok",
          "timestamp": 1707219911487,
          "user": {
            "displayName": "",
            "userId": ""
          },
          "user_tz": -480
        },
        "id": "oX--Ye5l1Yuq",
        "outputId": "6c8e2f1a-f5ec-4665-d078-c657e28bae91"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def download_random_images(num_images, folder_path):\n",
        "    for i in range(num_images):\n",
        "        response = requests.get(f\"https://picsum.photos/300/300?random={i}\")\n",
        "        filename = f\"{folder_path}/image_{i}.jpg\"\n",
        "        with open(filename, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        print(f\"Downloaded {filename}\")\n",
        "\n",
        "# Example usage\n",
        "num_images = 100\n",
        "download_random_images(num_images, folder_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cISKkZqJONq0",
      "metadata": {
        "id": "cISKkZqJONq0"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vFhAc6_78YOh",
        "aOtVAlUNQFtb",
        "5rXAPne-QJnU",
        "C6onP-miQS3F",
        "Ju0wKZftUrAL",
        "sAIXxBgQyLed",
        "PIBBvcCtyNN-",
        "IOoCIQsXTODH",
        "nC_ZPQ6LPxhd",
        "Fvly0b7hTIx5",
        "iDFUGBwW1Wvt"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "224c80a990fe45a38bcad9f536742970": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32d5f76b89e74c6d88205ed1015a34c5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59ec0ba9df724a02955de65de116901c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a87220be8ade4c62a0e891f32ea8076e",
              "IPY_MODEL_63f30df580a8438997fe6c96e60cca8d"
            ],
            "layout": "IPY_MODEL_224c80a990fe45a38bcad9f536742970"
          }
        },
        "63f30df580a8438997fe6c96e60cca8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b56695160b084e2d9c3ca920a2c10621",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fdbd2c368f7c4e26ac3cdf8c0247fe37",
            "value": 1
          }
        },
        "98be374439484985b7dd5e5fb53a88ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a87220be8ade4c62a0e891f32ea8076e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32d5f76b89e74c6d88205ed1015a34c5",
            "placeholder": "​",
            "style": "IPY_MODEL_98be374439484985b7dd5e5fb53a88ff",
            "value": "0.012 MB of 0.012 MB uploaded\r"
          }
        },
        "b56695160b084e2d9c3ca920a2c10621": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdbd2c368f7c4e26ac3cdf8c0247fe37": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
